{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. MovieData Relational Operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from omdb import OMDBClient\n",
    "client = OMDBClient(apikey='e55a0e19')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "# define input and output folder paths\n",
    "input_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files'\n",
    "output_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files'\n",
    "\n",
    "relational_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files\\relational'\n",
    "# staging_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files\\staging'\n",
    "\n",
    "user_created_data_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files\\user_created_moviedata'\n",
    "staging_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\4_staging'\n",
    "raw_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\3_raw'\n",
    "curated_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\2_curated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 7/16/23 copying entire block from Movie_Data_Relational_Nb_dev1a, but it still needs to be double-checked to eliminate functions not used in this notebook\n",
    "\n",
    "# helper function to search omdb for each film and return the unique imdb ID\n",
    "def get_film_object(row):\n",
    "    film_title = row['Title']\n",
    "    film_year_released = str(row['Year_Released'])\n",
    "\n",
    "    try:\n",
    "        film_object = client.search(film_title, year=film_year_released)\n",
    "        return film_object[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row: {row} - Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# helper function to extract the unique imdb ids from the list\n",
    "def extract_column_to_list(df, column_name):\n",
    "    return df[column_name].tolist()\n",
    "\n",
    "# helper function to create lists of tuples for imdb key and other dictionary keys\n",
    "def create_data_structure(item, key_name):\n",
    "    values = item[key_name].split(', ')\n",
    "    imdb_id = item['imdbID']\n",
    "    tuple_result = [(imdb_id, value) for value in values]\n",
    "    return tuple_result\n",
    "\n",
    "# helper function to flatten each ragged list\n",
    "def flatten_data_structure(main_list):\n",
    "    flattened_list = []\n",
    "    for sublist in main_list:\n",
    "        for item in sublist:\n",
    "            flattened_list.append(item)\n",
    "    return flattened_list\n",
    "\n",
    "# helper function to create lookup table\n",
    "def return_lookup_dataframe(ph_data_list, ph_key):\n",
    "    # Step 1: Create data structure\n",
    "    cat_obj = [create_data_structure(ph_dl, ph_key) for ph_dl in ph_data_list]\n",
    "    # Step 2: Flatten the data structure\n",
    "    ph_flattened_list = flatten_data_structure(cat_obj)\n",
    "    # Step 3: Create the lookup DataFrame\n",
    "    ph_lookup_df = pd.DataFrame(ph_flattened_list, columns=column_set)\n",
    "    # Step 4: Add key id column\n",
    "    ph_lookup_df['key'] = range(1, len(ph_lookup_df) + 1)\n",
    "    # Step 6: Add 'date_updated' column with current date\n",
    "    ph_lookup_df['date_updated'] = datetime.datetime.now().date()\n",
    "    # Step 5: Reorder the columns\n",
    "    ph_lookup_df = ph_lookup_df[['key', column_set[0], column_set[1], 'date_updated']]\n",
    "    # Step 6: Return dataframe\n",
    "    return ph_lookup_df\n",
    "\n",
    "    # this code block may serve a better purpose by returning a dataframe\n",
    "def return_distinct_dated_dataframe(ph_data_list, ph_key):\n",
    "    # Step 1: Create data structure\n",
    "    cat_obj = [create_data_structure(ph_dl, ph_key) for ph_dl in ph_data_list]\n",
    "    # Step 2: Flatten the data structure\n",
    "    ph_flattened_list = flatten_data_structure(cat_obj)\n",
    "    # Step 3: Create the distinct Dimension DataFrame\n",
    "    ph_dimension_df = pd.DataFrame(ph_flattened_list, columns=column_set)\n",
    "    ph_dimension_df.drop(columns=column_set[0], inplace=True)\n",
    "    # Step 4: Drop duplicates before adding key column\n",
    "    ph_dimension_df.drop_duplicates(subset=column_set[1], inplace=True)\n",
    "    # Step 5: Add id column\n",
    "    ph_dimension_df['id'] = range(1, len(ph_dimension_df) + 1)\n",
    "    # Step 6: Add 'date_updated' column with current date\n",
    "    ph_dimension_df['date_updated'] = datetime.datetime.now().date()\n",
    "    # Step 7: Reorder the columns\n",
    "    ph_dimension_df = ph_dimension_df[['id', column_set[1], 'date_updated']]\n",
    "     # Step 8: Return Dataframe\n",
    "    return ph_dimension_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. Import pickle file and load into dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# this reads a file that was created in Movies_Data_fromAPItoExport notebook\n",
    "import pickle\n",
    "\n",
    "# movie_data_list = []\n",
    "# the above empty list declaration can be removed if no side effects\n",
    "\n",
    "file_name = 'data_list.pkl'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "with open(output_file, 'rb') as fp:\n",
    "    movie_data_list = pickle.load(fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xa0 in position 1109: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m input_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(input_folder, file_name)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# read csv\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m inflation_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelimiter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m|\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m inflation_df\u001B[38;5;241m.\u001B[39mcolumns\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1753\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1750\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1752\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1753\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mapping[engine](f, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions)\n\u001B[0;32m   1754\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1755\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:79\u001B[0m, in \u001B[0;36mCParserWrapper.__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m     76\u001B[0m     kwds\u001B[38;5;241m.\u001B[39mpop(key, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     78\u001B[0m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m ensure_dtype_objs(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m---> 79\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m parsers\u001B[38;5;241m.\u001B[39mTextReader(src, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     81\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munnamed_cols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader\u001B[38;5;241m.\u001B[39munnamed_cols\n\u001B[0;32m     83\u001B[0m \u001B[38;5;66;03m# error: Cannot determine type of 'names'\u001B[39;00m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:547\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.__cinit__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:636\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._get_header\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\My\\Python\\venv\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1965\u001B[0m, in \u001B[0;36mpandas._libs.parsers.raise_parser_error\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xa0 in position 1109: invalid start byte"
     ]
    }
   ],
   "source": [
    "# 7/21/23 save this task for later\n",
    "# import inflation_table into lookup dataframe\n",
    "# create path and name variable\n",
    "file_name = 'inflation_table.csv'\n",
    "input_file = os.path.join(input_folder, file_name)\n",
    "# read csv\n",
    "inflation_df = pd.read_csv(input_file, delimiter='|')\n",
    "inflation_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# persist inflation dataframe\n",
    "# inflation_df.to_csv(output_file, sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 4. Clean and Transform dataframe while adding key and re-ordering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['Title', 'Year', 'Rated', 'Released', 'Runtime', 'Genre', 'Director',\n       'Writer', 'Actors', 'Plot', 'Language', 'Country', 'Awards', 'Poster',\n       'Ratings', 'Metascore', 'imdbRating', 'imdbVotes', 'imdbID', 'Type',\n       'DVD', 'BoxOffice', 'Production', 'Website', 'Response',\n       'InternetMovieDatabase', 'RottenTomatoes', 'Metacritic'],\n      dtype='object')"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create 3NF film table\n",
    "#make dataframe with minimal columns\n",
    "# movie_data_df is made using the full movie_data_list\n",
    "movie_data_df = pd.DataFrame(movie_data_list)\n",
    "movie_data_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<class 'str'>    71\nName: BoxOffice, dtype: int64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# experiment to check types\n",
    "\"\"\"\n",
    "trial_df = movie_data_df.copy()\n",
    "trial_df['BoxOffice'] = trial_df['BoxOffice'].replace('N/A','$0')\n",
    "# trial_df.dropna(subset=['BoxOffice'], inplace=True)\n",
    "trial_df['BoxOffice'].apply(type).value_counts()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# experiment to check types\n",
    "\"\"\"\n",
    "trial_df['BoxOffice'] = trial_df['BoxOffice'].str.strip()\n",
    "trial_df['BoxOffice'] = trial_df['BoxOffice'].str.replace(',','').str.replace('$','').astype('int')\n",
    "trial_df['BoxOffice'].apply(type).value_counts()\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# add id column\n",
    "movie_data_df['id'] = range(1, len(movie_data_df) + 1)\n",
    "# choose columns\n",
    "column_list=[\"id\",\"imdbID\",\"Title\",\"Year\",\"Rated\",\"Released\",\"Runtime\",\"Plot\",\"Poster\",\"Metascore\",\"Metacritic\",\"InternetMovieDatabase\",\"RottenTomatoes\",\"imdbRating\",\"imdbVotes\",\"Type\",\"DVD\",\"BoxOffice\"]\n",
    "# creating a narrow data frame just for films in the list\n",
    "film_data_df = pd.DataFrame(movie_data_df, columns=column_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Replace NaN values in specific columns of movie_data_df\n",
    "columns_to_fill = {\n",
    "    \"Released\": \"1/1/1900\",\n",
    "    \"DVD\": \"1/1/1900\",\n",
    "    \"Year\": \"1900\",\n",
    "    \"RottenTomatoes\": \"0%\",\n",
    "    \"BoxOffice\": \"$0\",\n",
    "    \"Metascore\": \"0\",\n",
    "    \"imdbRating\": \"0\",\n",
    "    \"imdbVotes\": \"0\"\n",
    "}\n",
    "\n",
    "na_value = 'N/A'\n",
    "\n",
    "for column, value in columns_to_fill.items():\n",
    "    film_data_df[column] = film_data_df[column].replace(na_value ,value)\n",
    "    # film_data_df[column] = film_data_df[column].fillna(value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "file_name = 'films.csv'\n",
    "output_file = os.path.join(relational_folder, file_name)\n",
    "film_data_df.to_csv(output_file, sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 5. Create master, relational, & lookup dataframes and exports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n",
      "     f.id  m.id date_updated_y\n",
      "0       1     1     2023-07-16\n",
      "1       1     2     2023-07-16\n",
      "2       1     3     2023-07-16\n",
      "3       2     4     2023-07-16\n",
      "4       2     5     2023-07-16\n",
      "..    ...   ...            ...\n",
      "208    68   141     2023-07-16\n",
      "209    68   146     2023-07-16\n",
      "210    65   141     2023-07-16\n",
      "211    65   142     2023-07-16\n",
      "212    65   143     2023-07-16\n",
      "\n",
      "[213 rows x 3 columns]\n",
      "stop\n",
      "    f.id  m.id date_updated_y\n",
      "0      1     1     2023-07-16\n",
      "1      2     2     2023-07-16\n",
      "2      3     2     2023-07-16\n",
      "3      4     3     2023-07-16\n",
      "4     64     3     2023-07-16\n",
      "..   ...   ...            ...\n",
      "80    62    57     2023-07-16\n",
      "81    63    58     2023-07-16\n",
      "82    69    59     2023-07-16\n",
      "83    70    60     2023-07-16\n",
      "84    71    61     2023-07-16\n",
      "\n",
      "[85 rows x 3 columns]\n",
      "stop\n",
      "     f.id  m.id date_updated_y\n",
      "0       1     1     2023-07-16\n",
      "1       1     2     2023-07-16\n",
      "2       2     3     2023-07-16\n",
      "3       2     4     2023-07-16\n",
      "4       2     5     2023-07-16\n",
      "..    ...   ...            ...\n",
      "159    69   110     2023-07-16\n",
      "160    70   111     2023-07-16\n",
      "161    70   112     2023-07-16\n",
      "162    70   113     2023-07-16\n",
      "163    71   114     2023-07-16\n",
      "\n",
      "[164 rows x 3 columns]\n",
      "stop\n",
      "     f.id  m.id date_updated_y\n",
      "0       1     1     2023-07-16\n",
      "1       1     2     2023-07-16\n",
      "2       1     3     2023-07-16\n",
      "3       4     1     2023-07-16\n",
      "4       4     2     2023-07-16\n",
      "..    ...   ...            ...\n",
      "193    58    10     2023-07-16\n",
      "194    58    11     2023-07-16\n",
      "195    26     9     2023-07-16\n",
      "196    26    12     2023-07-16\n",
      "197    36    11     2023-07-16\n",
      "\n",
      "[198 rows x 3 columns]\n",
      "stop\n",
      "     f.id  m.id date_updated_y\n",
      "0       1     1     2023-07-16\n",
      "1       1     2     2023-07-16\n",
      "2       1     3     2023-07-16\n",
      "3       2     1     2023-07-16\n",
      "4       2     4     2023-07-16\n",
      "..    ...   ...            ...\n",
      "178    71     1     2023-07-16\n",
      "179    71     2     2023-07-16\n",
      "180    71    26     2023-07-16\n",
      "181    71    42     2023-07-16\n",
      "182    71    43     2023-07-16\n",
      "\n",
      "[183 rows x 3 columns]\n",
      "stop\n",
      "     f.id  m.id date_updated_y\n",
      "0       1     1     2023-07-16\n",
      "1       1     2     2023-07-16\n",
      "2       1     3     2023-07-16\n",
      "3       1     4     2023-07-16\n",
      "4       5     1     2023-07-16\n",
      "..    ...   ...            ...\n",
      "135    70     2     2023-07-16\n",
      "136    70    13     2023-07-16\n",
      "137    70    16     2023-07-16\n",
      "138    71     2     2023-07-16\n",
      "139    25    13     2023-07-16\n",
      "\n",
      "[140 rows x 3 columns]\n",
      "stop\n",
      "    f.id  m.id date_updated_y\n",
      "0      1     1     2023-07-16\n",
      "1      2     2     2023-07-16\n",
      "2      3     3     2023-07-16\n",
      "3      4     4     2023-07-16\n",
      "4      5     5     2023-07-16\n",
      "..   ...   ...            ...\n",
      "67    67    68     2023-07-16\n",
      "68    68    69     2023-07-16\n",
      "69    69    70     2023-07-16\n",
      "70    70    71     2023-07-16\n",
      "71    71    72     2023-07-16\n",
      "\n",
      "[72 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "lookup_data = [\n",
    "    {\n",
    "        'file_name': 'film_actors.csv',\n",
    "        '3NF_file_name': 'actors.csv',\n",
    "        'column_set': ['imdb_id', 'actors'],\n",
    "        'table_name': 'Actors'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_directors.csv',\n",
    "        '3NF_file_name': 'directors.csv',\n",
    "        'column_set': ['imdb_id', 'director'],\n",
    "        'table_name': 'Director'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_writers.csv',\n",
    "        '3NF_file_name': 'writers.csv',\n",
    "        'column_set': ['imdb_id', 'writer'],\n",
    "        'table_name': 'Writer'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_genres.csv',\n",
    "        '3NF_file_name': 'genres.csv',\n",
    "        'column_set': ['imdb_id', 'genre'],\n",
    "        'table_name': 'Genre'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_languages.csv',\n",
    "        '3NF_file_name': 'languages.csv',\n",
    "        'column_set': ['imdb_id', 'language'],\n",
    "        'table_name': 'Language'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_countries.csv',\n",
    "        '3NF_file_name': 'countries.csv',\n",
    "        'column_set': ['imdb_id', 'country'],\n",
    "        'table_name': 'Country'\n",
    "    },\n",
    "    {\n",
    "        'file_name': 'film_titles.csv',\n",
    "        '3NF_file_name': 'titles.csv',\n",
    "        'column_set': ['imdb_id', 'title'],\n",
    "        'table_name': 'Title'\n",
    "    }\n",
    "]\n",
    "# the following for loop should include just returning a dataframe instead of making a table for a file export if it accomplishes a goal. The file exports may still be need outside the operational application\n",
    "for data in lookup_data:\n",
    "    file_name = data['file_name']\n",
    "    rel_file_name = data['3NF_file_name']\n",
    "    output_file = os.path.join(relational_folder, file_name)\n",
    "    column_set = data['column_set']\n",
    "    table_name = data['table_name']\n",
    "\n",
    "    lookup_df = return_lookup_dataframe(movie_data_list, table_name)\n",
    "    master_df = return_distinct_dated_dataframe(movie_data_list, table_name)\n",
    "    print(\"stop\")\n",
    "\n",
    "    # Joining master_data_df, lookup_df, and film_data_df\n",
    "    reference_lookup = master_df.merge(lookup_df, on=master_df.columns[1]).merge(film_data_df, left_on='imdb_id', right_on='imdbID')\n",
    "\n",
    "    # Selecting the desired columns\n",
    "    # reference_lookup = reference_lookup[['id_x', 'id_y']].rename(columns={'id_x': 'm.id', 'id_y': 'f.id'})\n",
    "    reference_lookup = reference_lookup[['id_y', 'id_x', 'date_updated_y']].rename(columns={'id_x': 'm.id', 'id_y': 'f.id'})\n",
    "    reference_lookup.sort_values(\"f.id\")\n",
    "\n",
    "\n",
    "    # Printing the resulting DataFrame\n",
    "    print(reference_lookup)\n",
    "\n",
    "    output_file = os.path.join(relational_folder, file_name)\n",
    "    reference_lookup.to_csv(output_file, sep='|', index=False)\n",
    "    output_file = os.path.join(relational_folder, rel_file_name)\n",
    "    master_df.to_csv(output_file, sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
