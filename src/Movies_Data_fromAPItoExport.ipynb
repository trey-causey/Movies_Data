{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## this is the first file that should be ran. next would be relational notebook that uses pickle file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setup imports and API key credentials"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports and client instance creation with apikey\n",
    "import os\n",
    "import omdb\n",
    "import pandas as pd\n",
    "from omdb import OMDBClient\n",
    "from datetime import datetime, timedelta\n",
    "client = OMDBClient(apikey='e55a0e19')\n",
    "\n",
    "# define input and output folder paths\n",
    "input_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files'\n",
    "output_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files'\n",
    "\n",
    "user_created_data_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\input_files\\user_created_moviedata'\n",
    "staging_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\4_staging'\n",
    "raw_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\3_raw'\n",
    "curated_folder = r'C:\\My\\Workspace\\Python_Projects\\Movies_Data\\output_files\\2_curated'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Define common functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# helper function to search omdb for each film and return the unique imdb ID\n",
    "# 7/22/23 may be able to add inventory_id here so that inventory file can be tied in with films file\n",
    "def get_film_object(row):\n",
    "    film_title = row['movie_title']\n",
    "    film_year_released = str(row['release_year'])\n",
    "    row_dict = row.to_dict()\n",
    "    expanded_dict = {}\n",
    "    try:\n",
    "        film_object = client.search(film_title, year=film_year_released)\n",
    "        # expanded_dict = row_dict.update(film_object[0])\n",
    "        return film_object[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row: {row} - Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# helper function to extract the unique imdb ids from the list\n",
    "def extract_column_to_list(df, column_name):\n",
    "    return df[column_name].tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. MovieData Operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE DOES NOT EXIST! A new one will be created\n"
     ]
    }
   ],
   "source": [
    "# 7/16/23 adding a copy of this here even though it needs work -its a good starting point. This should be modified as a check for changes area so the API is only hit for new entries\n",
    "# 7/17/23 in the long run this should turn into a staging table for get_film_object -> compare to existing -> fetch new\n",
    "\n",
    "# first, check for current MovieData file so the API can avoid getting hit\n",
    "file_name = 'data_list.pkl'\n",
    "output_file = os.path.join(input_folder, file_name)\n",
    "\n",
    "try:\n",
    "    if os.path.exists(output_file):\n",
    "        file_time = datetime.fromtimestamp(os.path.getmtime(output_file))\n",
    "        current_time = datetime.now()\n",
    "        time_difference = current_time - file_time\n",
    "        if time_difference.days > 2:\n",
    "            # File is more than 2 days old, perform necessary actions\n",
    "            print(\"Current File is outdated. Go ahead and create a new one\")\n",
    "        else:\n",
    "            # File is less than or equal to 2 days old, no action needed\n",
    "            print(\"File is less than 2 days old. Consider using the existing one\")\n",
    "    else:\n",
    "        # File does not exist, handle accordingly\n",
    "        print(\"FILE DOES NOT EXIST! A new one will be created\")\n",
    "except Exception as e:\n",
    "    # Exception occurred, handle the error\n",
    "    print(\"An error occurred:\", str(e))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create path and name variable\n",
    "file_name = 'film_list.csv'\n",
    "input_file = os.path.join(user_created_data_folder, file_name)\n",
    "# read csv\n",
    "csv_data = pd.read_csv(input_file, delimiter='|')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stops\n",
      "stops\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe and export of film_list as it since it contains other important data\n",
    "# column_list = [\"Inventory_Id\", \"Title\",\"Year_Released\",\"Watched\", \"4kUltra_Owned\",\"Bluray_Owned\",\"DVD_Owned\",\"Stream_Owned\"]\n",
    "column_list = [\"inventory_id\", \"movie_title\",\"release_year\",\"purchase_date\", \"watch_date\", \"4k_ultra_qty\",\"bluray_qty\",\"dvd_qty\",\"stream_qty\", \"watch_flag\"]\n",
    "inventory_df = csv_data[column_list]\n",
    "# create path and name variable\n",
    "file_name = 'movie_inventory.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "\n",
    "# this is to persist the dataframe\n",
    "inventory_df.to_csv(output_file, sep='|', index=False)\n",
    "print(\"stops\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# uses get_film_object helper function to find movie match\n",
    "csv_data['film_object'] = csv_data.apply(get_film_object, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# create path and name variable\n",
    "file_name = 'movie_inventory_multiline.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "\n",
    "# this is to persist the dataframe\n",
    "csv_data.to_csv(output_file, sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop\n"
     ]
    }
   ],
   "source": [
    "# should insert the convert inflation lookup table here. Can mimic the code for the film_list.csv to movie_inventory.csv functionality\n",
    "print(\"stop\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 7/16/23 adding this code from movies_data_nb_dev1a, but it could still use editing to keep only the essential parts\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# would like to make a non-key version of movie-list.csv so it can keep a working movie_list.csv->Movie_Data_DW.movies_staging->power bi report for the time being\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# value in csv_data['inventory_id'].items()\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# try to append inventory_id since it is a list\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m film_data \u001B[38;5;241m=\u001B[39m [value \u001B[38;5;28;01mfor\u001B[39;00m _, value \u001B[38;5;129;01min\u001B[39;00m csv_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilm_object\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# creates a dataframe with five columns (title, year, imdb_id, type, poster)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m full_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(film_data)\n",
      "Cell \u001B[1;32mIn[18], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 7/16/23 adding this code from movies_data_nb_dev1a, but it could still use editing to keep only the essential parts\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# would like to make a non-key version of movie-list.csv so it can keep a working movie_list.csv->Movie_Data_DW.movies_staging->power bi report for the time being\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# value in csv_data['inventory_id'].items()\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# try to append inventory_id since it is a list\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m film_data \u001B[38;5;241m=\u001B[39m [value \u001B[38;5;28;01mfor\u001B[39;00m _, value \u001B[38;5;129;01min\u001B[39;00m csv_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilm_object\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# creates a dataframe with five columns (title, year, imdb_id, type, poster)\u001B[39;00m\n\u001B[0;32m     10\u001B[0m full_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(film_data)\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:929\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:920\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:317\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\223.8214.51\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\JetBrains\\Toolbox\\apps\\PyCharm-P\\ch-0\\223.8214.51\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 7/16/23 adding this code from movies_data_nb_dev1a, but it could still use editing to keep only the essential parts\n",
    "\n",
    "# would like to make a non-key version of movie-list.csv so it can keep a working movie_list.csv->Movie_Data_DW.movies_staging->power bi report for the time being\n",
    "# continuation of above...this creates a list of dictionaries with 5 entries each (key, title, year, imdb_id, type, poster)\n",
    "# film_data = [value for _, value in csv_data['film_object'].items()]\n",
    "# value in csv_data['inventory_id'].items()\n",
    "# try to append inventory_id since it is a list\n",
    "film_data = [value for _, value in csv_data['film_object'].items()]\n",
    "# creates a dataframe with five columns (title, year, imdb_id, type, poster)\n",
    "full_df = pd.DataFrame(film_data)\n",
    "\n",
    "column_list = [\"title\", \"year\", \"imdb_id\", \"type\", \"poster\"]\n",
    "\n",
    "full_df = full_df[column_list]\n",
    "\n",
    "# create path and name variable\n",
    "file_name = 'movie_list.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "\n",
    "# this is to persist the dataframe, may not be necessary in databricks\n",
    "full_df.to_csv(output_file, sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# uses extract_column_to_list helper function to isolate the imdb_id column\n",
    "imdb_id_list = extract_column_to_list(full_df, 'imdb_id')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# create list of API endpoints for further lookups, example URL: https://www.omdbapi.com/?i=tt2382320&apikey=e55a0e19\n",
    "url_list = []\n",
    "import urllib.parse\n",
    "for imdbId in imdb_id_list:\n",
    "    url = 'https://www.omdbapi.com/?'\n",
    "    params = {'i': imdbId, 'apikey': 'e55a0e19'}\n",
    "    temp_url = str(url + urllib.parse.urlencode(params))\n",
    "    url_list.append(temp_url)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# create csv of endpoints...can do single list or can apply to an existing dataframe\n",
    "s_obj = pd.Series(url_list)\n",
    "# create path and name variable\n",
    "file_name = 'endpoint_list.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "# Save the DataFrame to a CSV file with no leading 0 and no quoting of fields\n",
    "s_obj.to_csv(output_file, header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# could make a more raw form of the results here\n",
    "import requests\n",
    "# import json\n",
    "data_list_raw = []\n",
    "for api_url in url_list:\n",
    "    resp = requests.get(api_url)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    # for d in data[\"Ratings\"]:\n",
    "    #     temp_dict = {d[\"Source\"].replace(\" \", \"\"):d[\"Value\"]}\n",
    "    #     data.update(temp_dict.items()) # data is a dict object with 28 entries each pass through the loop\n",
    "    data_list_raw.append(data) #data_list is a list of dictionaries that are created each pass\n",
    "\n",
    "#create a dataframe from the list\n",
    "data_list_raw_df = pd.DataFrame(data_list_raw)\n",
    "\n",
    "#save the entire set to file\n",
    "file_name = 'movie_list_raw_all_columns.json'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "data_list_raw_df.to_json(output_file, orient=\"table\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "#save the entire set to file\n",
    "file_name = 'movie_list_raw_all_columns.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "data_list_raw_df.to_csv(output_file,sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# create a list where each row is all the movie data found from each key\n",
    "import requests\n",
    "# import json\n",
    "data_list = []\n",
    "for api_url in url_list:\n",
    "    resp = requests.get(api_url)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    for d in data[\"Ratings\"]:\n",
    "        temp_dict = {d[\"Source\"].replace(\" \", \"\"):d[\"Value\"]}\n",
    "        data.update(temp_dict.items()) # data is a dict object with 28 entries each pass through the loop\n",
    "    data_list.append(data) #data_list is a list of dictionaries that are created each pass\n",
    "\n",
    "#create a dataframe from the pre-loaded list\n",
    "data_list_df = pd.DataFrame(data_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Export dataframe as is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save dictionary to data_list.pkl file\n",
    "file_name = 'data_list.pkl'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "with open(output_file, 'wb') as fp:\n",
    "    pickle.dump(data_list, fp)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "#save the entire set to file\n",
    "file_name = 'movie_list_all_columns.json'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "data_list_df.to_json(output_file, orient=\"table\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "#save the entire set to file\n",
    "file_name = 'movie_list_all_columns.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "data_list_df.to_csv(output_file,sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Clean and conform the DataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "column_list=[\"Title\",\"Year\",\"Rated\",\"Released\",\"Runtime\",\"Genre\",\"Director\",\"Writer\",\"Actors\",\"Plot\",\"Language\",\"Country\",\"Awards\",\"Poster\", \"Ratings\", \"Metascore\",\"imdbRating\",\"imdbVotes\",\"InternetMovieDatabase\",\"RottenTomatoes\",\"Metacritic\",\"imdbID\",\"Type\",\"DVD\",\"BoxOffice\",\"Production\",\"Website\"]\n",
    "# create a dataframe object based on the previous one where only the relevant columns are used\n",
    "data_list_df_partial = pd.DataFrame(data_list_df, columns=column_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert these to snake case at some point\n",
    "# column_list=[\"Title\",\"Year\",\"Rated\",\"Released\",\"Runtime\",\"Genre\",\"Director\",\"Writer\",\"Actors\",\"Plot\",\"Language\",\"Country\",\"Awards\",\"Poster\", \"Ratings\", \"Metascore\",\"imdbRating\",\"imdbVotes\",\"InternetMovieDatabase\",\"RottenTomatoes\",\"Metacritic\",\"imdbID\",\"Type\",\"DVD\",\"BoxOffice\",\"Production\",\"Website\"]\n",
    "# # create a dataframe object based on the previous one where only the relevant columns are used\n",
    "# data_list_df_partial = pd.DataFrame(data_list_df, columns=column_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "file_name = 'movie_list_detail.csv'\n",
    "output_file = os.path.join(staging_folder, file_name)\n",
    "data_list_df_partial.to_csv(output_file,sep='|', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
